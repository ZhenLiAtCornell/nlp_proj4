{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import timeit\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "import nltk\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"..\"\n",
    "train_path = os.path.join(data_dir, \"training.json\")\n",
    "dev_path = os.path.join(data_dir, \"development.json\")\n",
    "test_path = os.path.join(data_dir, \"test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(train_path) as fin:\n",
    "    j = json.load(fin)\n",
    "    \n",
    "train_raw = j[\"data\"]\n",
    "\n",
    "with open(dev_path) as fin:\n",
    "    j = json.load(fin)\n",
    "    \n",
    "dev_raw = j[\"data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212\n"
     ]
    }
   ],
   "source": [
    "tokset_stopwords = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "tokset_punct = set(string.punctuation)\n",
    "\n",
    "tokset_stopwords.update(tokset_punct)\n",
    "tokset_stopwords = sorted(tokset_stopwords)\n",
    "\n",
    "sample_dim = len(tokset_stopwords) + 1 # for label\n",
    "print(sample_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate np dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_np_data(data_raw):\n",
    "    \n",
    "    qids = set()\n",
    "    \n",
    "    for passage in data_raw:\n",
    "        title = passage['title']\n",
    "        paragraphs = passage['paragraphs']\n",
    "        for paragraph in paragraphs:\n",
    "            context = paragraph['context']\n",
    "            qas = paragraph['qas']\n",
    "            for qa in qas:\n",
    "                question = qa['question']\n",
    "                tokset_question = set(nltk.tokenize.word_tokenize(question.lower()))\n",
    "                qid = qa['id']\n",
    "                qids.add(qid)\n",
    "    \n",
    "    sample_num = len(qids)\n",
    "    \n",
    "    data = np.zeros((sample_num, sample_dim))\n",
    "    \n",
    "    i = 0\n",
    "    for passage in data_raw:\n",
    "        title = passage['title']\n",
    "        paragraphs = passage['paragraphs']\n",
    "        for paragraph in paragraphs:\n",
    "            context = paragraph['context']\n",
    "            qas = paragraph['qas']\n",
    "            for qa in qas:\n",
    "                question = qa['question']\n",
    "                tokset_question = set(nltk.tokenize.word_tokenize(question.lower()))\n",
    "                tokset_local = [tok for tok in tokset_question if tok in tokset_stopwords]\n",
    "                toknum_local = len(tokset_local) if tokset_local else 1\n",
    "                c = Counter(tokset_local)\n",
    "\n",
    "                for j, k in enumerate(tokset_stopwords):\n",
    "                    data[i, j] = c.get(k, 0) / toknum_local\n",
    "                data[i, -1] = int(qa[\"is_impossible\"])\n",
    "\n",
    "                if i % 10000 == 0:\n",
    "                    print(i)\n",
    "                i += 1    \n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNClassifier(nn.Module):\n",
    "    def __init__(self, hidden_num=1, dropout_p=None,\n",
    "                 input_dim=212, hidden_dim=256, class_num=2):\n",
    "        super(NNClassifier, self).__init__()\n",
    "        # loss\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        # hidden-hidden fcs\n",
    "        self.hiddens = [nn.Linear(input_dim, hidden_dim) for _ in range(hidden_num-1)]\n",
    "        # insert input-hidden fc\n",
    "        self.hiddens.insert(0, nn.Linear(input_dim, hidden_dim))\n",
    "        # dropout layers\n",
    "        self.dropout_p = dropout_p\n",
    "        if dropout_p is not None:\n",
    "            self.drops = [nn.Dropout(p=dropout_p) for _ in range(hidden_num)]\n",
    "        # output layer\n",
    "        self.out = nn.Linear(hidden_dim, class_num)\n",
    "        # dropout\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.hiddens)):\n",
    "            x = F.relu(self.hiddens[i](x))\n",
    "            if self.dropout_p is not None:\n",
    "                x = self.drops[i](x)\n",
    "        x = self.out(x)\n",
    "        x = x.squeeze()\n",
    "        val, idx = torch.max(x, dim=1)\n",
    "        return x, idx\n",
    "    \n",
    "    def compute_loss(self, pred_vec, gold_vec):\n",
    "        return self.loss(pred_vec, gold_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_a_try(X_train, X_test, Y_train, Y_test,\n",
    "               hidden_num, dropout_p, lr, epoch_num,\n",
    "               batch_size, debug_mode=True):\n",
    "    debug_report_seg = epoch_num // 10\n",
    "    train_size, input_dim = X_train.shape\n",
    "    model = NNClassifier(input_dim=input_dim, dropout_p=dropout_p)\n",
    "#     optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    optimizer.zero_grad()\n",
    "    model.train()\n",
    "    start_train = time.time()\n",
    "    batch_num = train_size // batch_size\n",
    "    for epoch in range(epoch_num):\n",
    "        acc = 0\n",
    "        loss = 0\n",
    "        correct = 0\n",
    "        p_total = 0\n",
    "        r_total = 0\n",
    "        for batch in range(batch_num):\n",
    "            X_batch = X_train[batch*batch_size:(batch+1)*batch_size]\n",
    "            Y_batch = Y_train[batch*batch_size:(batch+1)*batch_size]\n",
    "            optimizer.zero_grad()\n",
    "            inputs = torch.tensor(X_batch).float()\n",
    "            golds  = torch.tensor(Y_batch).long()\n",
    "            pred_vals, pred_labels  = model(inputs)\n",
    "            batch_loss = model.compute_loss(pred_vals, golds)\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if debug_mode and epoch % debug_report_seg == 0:\n",
    "#                 # debug only\n",
    "#                 print((pred_labels == golds and pred_labels == 1).sum())\n",
    "#                 raise RuntimeError(\"DEBUGGING\")\n",
    "                \n",
    "                \n",
    "                acc += golds.eq(pred_labels).sum().float() / batch_size\n",
    "                loss += batch_loss\n",
    "                pred_labels = pred_labels.numpy()\n",
    "                golds = golds.numpy()\n",
    "                correct += (np.logical_and(pred_labels == golds, pred_labels == 1)).sum()\n",
    "                p_total += (pred_labels == 1).sum()\n",
    "                r_total += (golds == 1).sum()\n",
    "\n",
    "        if debug_mode and epoch % debug_report_seg == 0:\n",
    "            p = correct / p_total\n",
    "            r = correct / r_total\n",
    "            f = 2*p*r / (p+r)\n",
    "            print(\"epoch {:>5d}, loss = {:.4f}, p = {:.4f}, r = {:.4f}, f = {:.4f}, acc = {:.6f}\"\\\n",
    "                  .format(epoch, loss/batch_num, p, r, f, acc/batch_num))\n",
    "\n",
    "\n",
    "#     acc = golds.eq(pred_labels).sum().float() / train_size\n",
    "#     print(\"training: loss = {}, acc = {}\".format(loss, acc))\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    inputs = torch.tensor(X_train).float()\n",
    "    golds  = torch.tensor(Y_train).long()\n",
    "    pred_vals, pred_labels  = model(inputs)\n",
    "    loss = model.compute_loss(pred_vals, golds)\n",
    "    acc = golds.eq(pred_labels).sum().float() / train_size\n",
    "    print(\"training: loss = {}, acc = {}\".format(loss, acc))\n",
    "\n",
    "    \n",
    "    test_size, input_dim = X_test.shape\n",
    "    inputs = torch.tensor(X_test).float()\n",
    "    golds  = torch.tensor(Y_test).long()\n",
    "    pred_vals, pred_labels  = model(inputs)\n",
    "    loss = model.compute_loss(pred_vals, golds)\n",
    "    acc = golds.eq(pred_labels).sum().float() / test_size\n",
    "    print(\"test: loss = {}, acc = {}\".format(loss, acc))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n"
     ]
    }
   ],
   "source": [
    "train_np = gen_np_data(train_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0.0, 1.0}\n",
      "(100814, 212)\n"
     ]
    }
   ],
   "source": [
    "print(set(train_np[:, -1].tolist()))\n",
    "print(train_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "20000\n"
     ]
    }
   ],
   "source": [
    "dev_np = gen_np_data(dev_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0.0, 1.0}\n",
      "(29505, 212)\n"
     ]
    }
   ],
   "source": [
    "print(set(dev_np[:, -1].tolist()))\n",
    "print(dev_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_np[:, :-1]\n",
    "Y_train = train_np[:, -1]\n",
    "X_dev = dev_np[:, :-1]\n",
    "Y_dev = dev_np[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch     0, loss = 0.6866, p = 0.2641, r = 0.1624, f = 0.2011, acc = 0.567940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zhenl\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:45: RuntimeWarning: invalid value encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch     5, loss = 0.6387, p = nan, r = 0.0000, f = nan, acc = 0.665100\n",
      "epoch    10, loss = 0.6373, p = nan, r = 0.0000, f = nan, acc = 0.665100\n",
      "epoch    15, loss = 0.6371, p = nan, r = 0.0000, f = nan, acc = 0.665100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-2ed3754b4189>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m model_ch = make_a_try(X_train, X_dev, Y_train, Y_dev, \n\u001b[0;32m      3\u001b[0m                       \u001b[0mhidden_num\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout_p\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_num\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m                       batch_size=5000, debug_mode=True)\n\u001b[0m",
      "\u001b[1;32m<ipython-input-25-ceeb5202d42d>\u001b[0m in \u001b[0;36mmake_a_try\u001b[1;34m(X_train, X_test, Y_train, Y_test, hidden_num, dropout_p, lr, epoch_num, batch_size, debug_mode)\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m             \u001b[0mgolds\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m             \u001b[0mpred_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred_labels\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[0mbatch_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgolds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 1 layer, no dropout\n",
    "model_ch = make_a_try(X_train, X_dev, Y_train, Y_dev, \n",
    "                      hidden_num=1, dropout_p=None, lr=1, epoch_num=50,\n",
    "                      batch_size=5000, debug_mode=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### count of stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "47000\n",
      "48000\n",
      "49000\n",
      "50000\n",
      "51000\n",
      "52000\n",
      "53000\n",
      "54000\n",
      "55000\n",
      "56000\n",
      "57000\n",
      "58000\n",
      "59000\n",
      "60000\n",
      "61000\n",
      "62000\n",
      "63000\n",
      "64000\n",
      "65000\n",
      "66000\n",
      "67000\n",
      "68000\n",
      "69000\n",
      "70000\n",
      "71000\n",
      "72000\n",
      "73000\n",
      "74000\n",
      "75000\n",
      "76000\n",
      "77000\n",
      "78000\n",
      "79000\n",
      "80000\n",
      "81000\n",
      "82000\n",
      "83000\n",
      "84000\n",
      "85000\n",
      "86000\n",
      "87000\n",
      "88000\n",
      "89000\n",
      "90000\n",
      "91000\n",
      "92000\n",
      "93000\n",
      "94000\n",
      "95000\n",
      "96000\n",
      "97000\n",
      "98000\n",
      "99000\n",
      "100000\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "c0 = Counter()\n",
    "tot0 = 0\n",
    "c1 = Counter()\n",
    "tot1 = 0\n",
    "\n",
    "i = 0\n",
    "\n",
    "for passage in data:\n",
    "    title = passage['title']\n",
    "    paragraphs = passage['paragraphs']\n",
    "    for paragraph in paragraphs:\n",
    "        context = paragraph['context']\n",
    "        qas = paragraph['qas']\n",
    "        for qa in qas:\n",
    "            question = qa['question']\n",
    "            tokset_question = set(nltk.tokenize.word_tokenize(question.lower()))\n",
    "            qid = qa['id']\n",
    "            \n",
    "            c = Counter((tok for tok in tokset_question if tok in tokset_stopwords))\n",
    "            \n",
    "            label = int(qa[\"is_impossible\"])\n",
    "            if label == 1:\n",
    "                c1 += c\n",
    "                tot1 += 1\n",
    "            else:\n",
    "                c0 += c\n",
    "                tot0 += 1\n",
    "\n",
    "            if i % 1000 == 0:\n",
    "                print(i)\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         ! : 0.0002, 0.0001, 0.0001\n",
      "         # : 0.0001, 0.0003, -0.0001\n",
      "         $ : 0.0005, 0.0013, -0.0008\n",
      "         % : 0.0016, 0.0037, -0.0020\n",
      "         & : 0.0014, 0.0012, 0.0002\n",
      "         ' : 0.0094, 0.0043, 0.0051\n",
      "         ( : 0.0014, 0.0011, 0.0003\n",
      "         ) : 0.0014, 0.0011, 0.0002\n",
      "         * : 0.0000, 0.0000, 0.0000\n",
      "         + : 0.0001, 0.0001, -0.0000\n",
      "         , : 0.0672, 0.0397, 0.0274\n",
      "         - : 0.0003, 0.0003, 0.0001\n",
      "         . : 0.0042, 0.0023, 0.0019\n",
      "         / : 0.0000, 0.0000, 0.0000\n",
      "         : : 0.0007, 0.0008, -0.0001\n",
      "         ; : 0.0003, 0.0003, -0.0000\n",
      "         < : 0.0000, 0.0000, 0.0000\n",
      "         = : 0.0000, 0.0001, -0.0000\n",
      "         > : 0.0002, 0.0001, 0.0001\n",
      "         ? : 0.9892, 0.9922, -0.0030\n",
      "         [ : 0.0001, 0.0000, 0.0001\n",
      "         \\ : 0.0000, 0.0000, -0.0000\n",
      "         ] : 0.0001, 0.0000, 0.0001\n",
      "         ` : 0.0000, 0.0000, 0.0000\n",
      "         a : 0.1168, 0.1128, 0.0040\n",
      "     about : 0.0126, 0.0122, 0.0004\n",
      "     above : 0.0008, 0.0011, -0.0003\n",
      "     after : 0.0168, 0.0139, 0.0028\n",
      "     again : 0.0006, 0.0005, 0.0001\n",
      "   against : 0.0056, 0.0051, 0.0006\n",
      "       ain : 0.0000, 0.0000, 0.0000\n",
      "       all : 0.0068, 0.0079, -0.0011\n",
      "        am : 0.0001, 0.0002, -0.0001\n",
      "        an : 0.0259, 0.0267, -0.0008\n",
      "       and : 0.0792, 0.0673, 0.0119\n",
      "       any : 0.0026, 0.0018, 0.0008\n",
      "       are : 0.0631, 0.0686, -0.0055\n",
      "        as : 0.0424, 0.0366, 0.0058\n",
      "        at : 0.0288, 0.0228, 0.0059\n",
      "        be : 0.0338, 0.0332, 0.0007\n",
      "   because : 0.0027, 0.0019, 0.0008\n",
      "      been : 0.0109, 0.0128, -0.0019\n",
      "    before : 0.0053, 0.0073, -0.0020\n",
      "     being : 0.0072, 0.0057, 0.0015\n",
      "     below : 0.0006, 0.0007, -0.0001\n",
      "   between : 0.0115, 0.0107, 0.0008\n",
      "      both : 0.0025, 0.0020, 0.0005\n",
      "       but : 0.0020, 0.0020, -0.0000\n",
      "        by : 0.0440, 0.0472, -0.0032\n",
      "       can : 0.0196, 0.0189, 0.0007\n",
      "         d : 0.0001, 0.0002, -0.0001\n",
      "       did : 0.1832, 0.1749, 0.0083\n",
      "        do : 0.0345, 0.0433, -0.0088\n",
      "      does : 0.0512, 0.0625, -0.0113\n",
      "     doesn : 0.0000, 0.0000, -0.0000\n",
      "     doing : 0.0012, 0.0014, -0.0002\n",
      "       don : 0.0001, 0.0000, 0.0001\n",
      "      down : 0.0022, 0.0022, -0.0001\n",
      "    during : 0.0200, 0.0160, 0.0040\n",
      "      each : 0.0040, 0.0041, -0.0001\n",
      "       few : 0.0006, 0.0015, -0.0009\n",
      "       for : 0.0843, 0.0767, 0.0076\n",
      "      from : 0.0450, 0.0448, 0.0002\n",
      "   further : 0.0006, 0.0007, -0.0001\n",
      "       had : 0.0118, 0.0105, 0.0014\n",
      "       has : 0.0218, 0.0296, -0.0078\n",
      "      have : 0.0345, 0.0377, -0.0032\n",
      "     haven : 0.0043, 0.0001, 0.0041\n",
      "    having : 0.0015, 0.0014, 0.0001\n",
      "        he : 0.0060, 0.0034, 0.0025\n",
      "       her : 0.0040, 0.0015, 0.0025\n",
      "      here : 0.0001, 0.0001, 0.0000\n",
      "   herself : 0.0002, 0.0001, 0.0001\n",
      "       him : 0.0014, 0.0007, 0.0008\n",
      "   himself : 0.0007, 0.0003, 0.0003\n",
      "       his : 0.0150, 0.0091, 0.0059\n",
      "       how : 0.1099, 0.0943, 0.0156\n",
      "         i : 0.0018, 0.0021, -0.0003\n",
      "        if : 0.0050, 0.0048, 0.0002\n",
      "        in : 0.2777, 0.2766, 0.0011\n",
      "      into : 0.0104, 0.0103, 0.0002\n",
      "        is : 0.1829, 0.1857, -0.0028\n",
      "        it : 0.0175, 0.0155, 0.0020\n",
      "       its : 0.0123, 0.0118, 0.0005\n",
      "    itself : 0.0011, 0.0008, 0.0003\n",
      "      just : 0.0007, 0.0007, -0.0001\n",
      "        ll : 0.0000, 0.0000, 0.0000\n",
      "         m : 0.0001, 0.0001, 0.0000\n",
      "        ma : 0.0001, 0.0002, -0.0001\n",
      "        me : 0.0001, 0.0001, 0.0001\n",
      "      more : 0.0102, 0.0090, 0.0012\n",
      "      most : 0.0204, 0.0143, 0.0060\n",
      "        my : 0.0002, 0.0002, 0.0000\n",
      "        no : 0.0028, 0.0090, -0.0062\n",
      "       nor : 0.0001, 0.0001, 0.0000\n",
      "       not : 0.0138, 0.0546, -0.0409\n",
      "       now : 0.0024, 0.0018, 0.0006\n",
      "         o : 0.0002, 0.0003, -0.0001\n",
      "        of : 0.3433, 0.3058, 0.0376\n",
      "       off : 0.0017, 0.0013, 0.0004\n",
      "        on : 0.0538, 0.0498, 0.0040\n",
      "      once : 0.0014, 0.0009, 0.0005\n",
      "      only : 0.0054, 0.0090, -0.0036\n",
      "        or : 0.0132, 0.0079, 0.0053\n",
      "     other : 0.0173, 0.0139, 0.0034\n",
      "       our : 0.0004, 0.0004, -0.0000\n",
      "       out : 0.0061, 0.0052, 0.0009\n",
      "      over : 0.0073, 0.0069, 0.0005\n",
      "       own : 0.0020, 0.0022, -0.0002\n",
      "        re : 0.0001, 0.0001, 0.0000\n",
      "         s : 0.0004, 0.0007, -0.0003\n",
      "      same : 0.0025, 0.0035, -0.0010\n",
      "      shan : 0.0001, 0.0000, 0.0001\n",
      "       she : 0.0019, 0.0009, 0.0010\n",
      "    should : 0.0026, 0.0029, -0.0003\n",
      "        so : 0.0016, 0.0013, 0.0003\n",
      "      some : 0.0102, 0.0072, 0.0030\n",
      "      such : 0.0013, 0.0008, 0.0004\n",
      "         t : 0.0002, 0.0004, -0.0002\n",
      "      than : 0.0086, 0.0103, -0.0018\n",
      "      that : 0.0502, 0.0400, 0.0102\n",
      "       the : 0.5582, 0.5130, 0.0452\n",
      "     their : 0.0168, 0.0145, 0.0022\n",
      "    theirs : 0.0000, 0.0000, 0.0000\n",
      "      them : 0.0025, 0.0016, 0.0009\n",
      "themselves : 0.0014, 0.0012, 0.0002\n",
      "      then : 0.0006, 0.0007, -0.0001\n",
      "     there : 0.0089, 0.0119, -0.0030\n",
      "     these : 0.0032, 0.0021, 0.0011\n",
      "      they : 0.0067, 0.0060, 0.0008\n",
      "      this : 0.0094, 0.0052, 0.0042\n",
      "     those : 0.0015, 0.0014, 0.0001\n",
      "   through : 0.0041, 0.0041, -0.0000\n",
      "        to : 0.1958, 0.1864, 0.0094\n",
      "       too : 0.0008, 0.0009, -0.0001\n",
      "     under : 0.0062, 0.0051, 0.0011\n",
      "     until : 0.0024, 0.0027, -0.0003\n",
      "        up : 0.0082, 0.0072, 0.0011\n",
      "      very : 0.0014, 0.0019, -0.0005\n",
      "       was : 0.1954, 0.1791, 0.0162\n",
      "        we : 0.0011, 0.0009, 0.0001\n",
      "      were : 0.0517, 0.0425, 0.0092\n",
      "      what : 0.5794, 0.6297, -0.0503\n",
      "      when : 0.0771, 0.0731, 0.0040\n",
      "     where : 0.0447, 0.0384, 0.0063\n",
      "     which : 0.0734, 0.0533, 0.0201\n",
      "     while : 0.0018, 0.0016, 0.0002\n",
      "       who : 0.1044, 0.1115, -0.0071\n",
      "      whom : 0.0040, 0.0025, 0.0016\n",
      "       why : 0.0142, 0.0165, -0.0022\n",
      "      will : 0.0036, 0.0047, -0.0012\n",
      "      with : 0.0478, 0.0449, 0.0029\n",
      "       won : 0.0028, 0.0022, 0.0006\n",
      "    wouldn : 0.0000, 0.0000, 0.0000\n",
      "         y : 0.0002, 0.0001, 0.0001\n",
      "       you : 0.0026, 0.0033, -0.0007\n",
      "      your : 0.0004, 0.0007, -0.0003\n",
      "  yourself : 0.0000, 0.0000, 0.0000\n",
      "         { : 0.0000, 0.0000, -0.0000\n",
      "         } : 0.0000, 0.0000, -0.0000\n"
     ]
    }
   ],
   "source": [
    "keys = set(c0.keys())\n",
    "keys.update(c1.keys())\n",
    "keys = sorted(keys)\n",
    "\n",
    "for k in keys:\n",
    "    print(f\"{k:>10s} : {c0[k]/tot0:.4f}, {c1[k]/tot1:.4f}, {c0[k]/tot0-c1[k]/tot1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "211"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokset_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zhenl\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORD2VEC_PATH = \"we/GoogleNews-vectors-negative300.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = gensim.models.KeyedVectors.load_word2vec_format(WORD2VEC_PATH, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "we_len = 300\n",
    "def get_vector(word):\n",
    "    global w2v, we_len\n",
    "    return w2v.get_vector(word) if word in w2v.vocab \\\n",
    "            else np.zeros((we_len, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
