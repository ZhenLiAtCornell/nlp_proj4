{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### idea\n",
    "\n",
    "For each question, calculate \"how many words (in portion) in the question can be found in the context\".\n",
    "\n",
    "The context is defined as the passage title and the paragraph content.\n",
    "\n",
    "Further, stop words are eliminated from all texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import timeit\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"..\"\n",
    "train_path = os.path.join(data_dir, \"training.json\")\n",
    "dev_path = os.path.join(data_dir, \"development.json\")\n",
    "test_path = os.path.join(data_dir, \"testing.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"baseline_stats.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.07579107286924916, 2.563369440365878e-128)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.pearsonr(df[\"ratio\"], df[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00: p=0.34785, r=0.88662, f=0.49967\n",
      "0.05: p=0.34786, r=0.88662, f=0.49968\n",
      "0.10: p=0.34875, r=0.88438, f=0.50024\n",
      "0.15: p=0.35186, r=0.85166, f=0.49798\n",
      "0.20: p=0.35483, r=0.74005, f=0.47967\n",
      "0.25: p=0.35633, r=0.63605, f=0.45676\n",
      "0.30: p=0.35983, r=0.60439, f=0.45110\n",
      "0.35: p=0.36184, r=0.47964, f=0.41250\n",
      "0.40: p=0.36584, r=0.38813, f=0.37666\n",
      "0.45: p=0.37275, r=0.35701, f=0.36471\n",
      "0.50: p=0.37600, r=0.21035, f=0.26978\n",
      "0.55: p=0.37690, r=0.20958, f=0.26937\n",
      "0.60: p=0.38638, r=0.14323, f=0.20899\n",
      "0.65: p=0.39229, r=0.13693, f=0.20301\n",
      "0.70: p=0.39548, r=0.08220, f=0.13610\n",
      "0.75: p=0.40126, r=0.04522, f=0.08128\n",
      "0.80: p=0.41554, r=0.03003, f=0.05601\n",
      "0.85: p=0.44233, r=0.02403, f=0.04558\n",
      "0.90: p=0.48374, r=0.02066, f=0.03963\n",
      "0.95: p=0.48466, r=0.02054, f=0.03941\n"
     ]
    }
   ],
   "source": [
    "label = df[\"label\"].values\n",
    "\n",
    "for threshold in range(20):\n",
    "    threshold /= 20\n",
    "    \n",
    "    pred = (df[\"ratio\"].values > threshold).astype(int)\n",
    "    \n",
    "    correct = (np.logical_and(pred == label, label == 1)).sum()\n",
    "    p_total = (pred == 1).sum()\n",
    "    r_total = (label == 1).sum()\n",
    "    \n",
    "    p = correct / p_total\n",
    "    r = correct / r_total\n",
    "    f = 2*p*r / (p+r)\n",
    "\n",
    "#     p_tot = 0\n",
    "#     r_tot = 0\n",
    "#     p_cor = 0\n",
    "#     r_cor = 0\n",
    "    \n",
    "#     for i in range(len(df)):\n",
    "#         ratio = df.loc[i][\"ratio\"]\n",
    "#         label = df.loc[i][\"label\"]\n",
    "        \n",
    "#         pred = ratio > threshold\n",
    "        \n",
    "#         if pred == 1:\n",
    "#             p_tot += 1\n",
    "#             if label == 1:\n",
    "#                 p_cor += 1\n",
    "#         if label == 1:\n",
    "#             r_tot += 1\n",
    "#             if pred == 1:\n",
    "#                 r_cor += 1\n",
    "        \n",
    "#         if i % 10000 == 0:\n",
    "#             print(i)\n",
    "    print(f\"{threshold:.2f}: p={p:.5f}, r={r:.5f}, f={f:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6639554030194219"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratio = (df[\"ratio\"].values == 1).astype(int)\n",
    "(ratio==label).sum() / len(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## development accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "10000\n",
      "15000\n",
      "20000\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "with open(dev_path) as fin:\n",
    "    j = json.load(fin)\n",
    "\n",
    "data = j['data']\n",
    "    \n",
    "tokset_stopwords = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "tokset_punct = set(string.punctuation)\n",
    "\n",
    "i = 0\n",
    "\n",
    "d = {}\n",
    "\n",
    "for passage in data:\n",
    "    title = passage['title']\n",
    "    tokset_title = set(nltk.tokenize.word_tokenize(title.lower()))\n",
    "    paragraphs = passage['paragraphs']\n",
    "    for paragraph in paragraphs:\n",
    "        context = paragraph['context']\n",
    "        tokset_context = set(nltk.tokenize.word_tokenize(context.lower()))\n",
    "        qas = paragraph['qas']\n",
    "        for qa in qas:\n",
    "            question = qa['question']\n",
    "            tokset_question = set(nltk.tokenize.word_tokenize(question.lower()))\n",
    "            qid = qa['id']\n",
    "            tokset_non_stop = tokset_question - tokset_punct - tokset_stopwords\n",
    "            tokset_not_overlap = tokset_non_stop - tokset_title - tokset_context\n",
    "            non_stop_count = len(tokset_non_stop)\n",
    "            not_overlap_count = len(tokset_not_overlap)\n",
    "            ratio = not_overlap_count/non_stop_count if non_stop_count != 0 else 1\n",
    "            label = int(qa[\"is_impossible\"])\n",
    "\n",
    "            d[qid] = int(ratio > 0.50)\n",
    "            i += 1\n",
    "\n",
    "            if i % 5000 == 0:\n",
    "                print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_out_path = \"dev_out.json\"\n",
    "with open(dev_out_path, \"w\") as fout:\n",
    "    json.dump(d, fout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gen test output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "with open(test_path) as fin:\n",
    "    j = json.load(fin)\n",
    "\n",
    "data = j['data']\n",
    "    \n",
    "tokset_stopwords = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "tokset_punct = set(string.punctuation)\n",
    "\n",
    "i = 0\n",
    "\n",
    "d = {}\n",
    "\n",
    "for passage in data:\n",
    "    title = passage['title']\n",
    "    tokset_title = set(nltk.tokenize.word_tokenize(title.lower()))\n",
    "    paragraphs = passage['paragraphs']\n",
    "    for paragraph in paragraphs:\n",
    "        context = paragraph['context']\n",
    "        tokset_context = set(nltk.tokenize.word_tokenize(context.lower()))\n",
    "        qas = paragraph['qas']\n",
    "        for qa in qas:\n",
    "            question = qa['question']\n",
    "            tokset_question = set(nltk.tokenize.word_tokenize(question.lower()))\n",
    "            qid = qa['id']\n",
    "            tokset_non_stop = tokset_question - tokset_punct - tokset_stopwords\n",
    "            tokset_not_overlap = tokset_non_stop - tokset_title - tokset_context\n",
    "            non_stop_count = len(tokset_non_stop)\n",
    "            not_overlap_count = len(tokset_not_overlap)\n",
    "            ratio = not_overlap_count/non_stop_count if non_stop_count != 0 else 1\n",
    "#             label = int(qa[\"is_impossible\"])\n",
    "\n",
    "            d[qid] = int(ratio > 0.1)\n",
    "            i += 1\n",
    "\n",
    "            if i % 5000 == 0:\n",
    "                print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_out_path = \"test_out.json\"\n",
    "with open(test_out_path, \"w\") as fout:\n",
    "    json.dump(d, fout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test output to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_out_path = \"test_out.json\"\n",
    "with open(test_out_path) as fin:\n",
    "    j = json.load(fin)\n",
    "    \n",
    "df = pd.DataFrame(columns=[\"Predicted\"])\n",
    "df.index.name = \"Id\"\n",
    "\n",
    "for k, v in j.items():\n",
    "    df.loc[k] = v\n",
    "\n",
    "df.to_csv(\"test_out.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (DO NOT USE) work-around of `unable to find 17 required key values ...`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_sample = pd.read_csv(\"sample.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_out_path = \"test_out.json\"\n",
    "with open(test_out_path) as fin:\n",
    "    j = json.load(fin)\n",
    "    \n",
    "for k in kaggle_sample.index:\n",
    "    if k not in j:\n",
    "        j[k] = 0\n",
    "\n",
    "df = pd.DataFrame(columns=[\"Predicted\"])\n",
    "df.index.name = \"Id\"\n",
    "\n",
    "for k, v in j.items():\n",
    "    df.loc[k] = v\n",
    "\n",
    "df.to_csv(\"test_out.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17400\n",
      "11856\n"
     ]
    }
   ],
   "source": [
    "test_out_path = \"test_out.json\"\n",
    "with open(test_out_path) as fin:\n",
    "    j = json.load(fin)\n",
    "\n",
    "cnt = 0\n",
    "for k in kaggle_sample.index:\n",
    "    if k not in j:\n",
    "        cnt += 1\n",
    "        \n",
    "print(cnt)\n",
    "print(len(j))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Stats Gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(train_path) as fin:\n",
    "    j = json.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = j[\"data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=[\"id\", \"non_stop_count\", \"not_overlap_count\", \"ratio\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f():\n",
    "\n",
    "    tokset_stopwords = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "    tokset_punct = set(string.punctuation)\n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    for passage in data:\n",
    "        title = passage['title']\n",
    "        tokset_title = set(nltk.tokenize.word_tokenize(title.lower()))\n",
    "        paragraphs = passage['paragraphs']\n",
    "        for paragraph in paragraphs:\n",
    "            context = paragraph['context']\n",
    "            tokset_context = set(nltk.tokenize.word_tokenize(context.lower()))\n",
    "            qas = paragraph['qas']\n",
    "            for qa in qas:\n",
    "                question = qa['question']\n",
    "                tokset_question = set(nltk.tokenize.word_tokenize(question.lower()))\n",
    "                qid = qa['id']\n",
    "                tokset_non_stop = tokset_question - tokset_punct - tokset_stopwords\n",
    "                tokset_not_overlap = tokset_non_stop - tokset_title - tokset_context\n",
    "                non_stop_count = len(tokset_non_stop)\n",
    "                not_overlap_count = len(tokset_not_overlap)\n",
    "                ratio = not_overlap_count/non_stop_count if non_stop_count != 0 else 1\n",
    "                label = int(qa[\"is_impossible\"])\n",
    "                \n",
    "                df.loc[i] = (qid, non_stop_count, not_overlap_count, ratio, label)\n",
    "                i += 1\n",
    "                \n",
    "                if i % 1000 == 0:\n",
    "                    print(i)\n",
    "\n",
    "                # for debug\n",
    "#                 print(tokset_title)\n",
    "#                 print(tokset_context)\n",
    "#                 print(tokset_question)\n",
    "#                 print(tokset_non_stop)\n",
    "#                 print(tokset_not_overlap)\n",
    "#                 print(ratio)\n",
    "#                 print(label)\n",
    "#                 return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "47000\n",
      "48000\n",
      "49000\n",
      "50000\n",
      "51000\n",
      "52000\n",
      "53000\n",
      "54000\n",
      "55000\n",
      "56000\n",
      "57000\n",
      "58000\n",
      "59000\n",
      "60000\n",
      "61000\n",
      "62000\n",
      "63000\n",
      "64000\n",
      "65000\n",
      "66000\n",
      "67000\n",
      "68000\n",
      "69000\n",
      "70000\n",
      "71000\n",
      "72000\n",
      "73000\n",
      "74000\n",
      "75000\n",
      "76000\n",
      "77000\n",
      "78000\n",
      "79000\n",
      "80000\n",
      "81000\n",
      "82000\n",
      "83000\n",
      "84000\n",
      "85000\n",
      "86000\n",
      "87000\n",
      "88000\n",
      "89000\n",
      "90000\n",
      "91000\n",
      "92000\n",
      "93000\n",
      "94000\n",
      "95000\n",
      "96000\n",
      "97000\n",
      "98000\n",
      "99000\n",
      "100000\n"
     ]
    }
   ],
   "source": [
    "f()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"baseline_stats.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Depricated, DO NOT USE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "passage = data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = passage['title']\n",
    "paragraphs = passage['paragraphs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = paragraphs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = paragraph['context']\n",
    "qas = paragraph['qas']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = qas[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = qa['question']\n",
    "qid = qa['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Beyoncé'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time. Their hiatus saw the release of Beyoncé\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'When did Beyoncé release Dangerously in Love?'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "toks_title = nltk.tokenize.word_tokenize(title.lower())\n",
    "toks_context = nltk.tokenize.word_tokenize(context.lower())\n",
    "toks_question = nltk.tokenize.word_tokenize(question.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['beyoncé']\n",
      "['beyoncé', 'giselle', 'knowles-carter', '(', '/biːˈjɒnseɪ/', 'bee-yon-say', ')', '(', 'born', 'september', '4', ',', '1981', ')', 'is', 'an', 'american', 'singer', ',', 'songwriter', ',', 'record', 'producer', 'and', 'actress', '.', 'born', 'and', 'raised', 'in', 'houston', ',', 'texas', ',', 'she', 'performed', 'in', 'various', 'singing', 'and', 'dancing', 'competitions', 'as', 'a', 'child', ',', 'and', 'rose', 'to', 'fame', 'in', 'the', 'late', '1990s', 'as', 'lead', 'singer', 'of', 'r', '&', 'b', 'girl-group', 'destiny', \"'s\", 'child', '.', 'managed', 'by', 'her', 'father', ',', 'mathew', 'knowles', ',', 'the', 'group', 'became', 'one', 'of', 'the', 'world', \"'s\", 'best-selling', 'girl', 'groups', 'of', 'all', 'time', '.', 'their', 'hiatus', 'saw', 'the', 'release', 'of', 'beyoncé', \"'s\", 'debut', 'album', ',', 'dangerously', 'in', 'love', '(', '2003', ')', ',', 'which', 'established', 'her', 'as', 'a', 'solo', 'artist', 'worldwide', ',', 'earned', 'five', 'grammy', 'awards', 'and', 'featured', 'the', 'billboard', 'hot', '100', 'number-one', 'singles', '``', 'crazy', 'in', 'love', \"''\", 'and', '``', 'baby', 'boy', \"''\", '.']\n",
      "['when', 'did', 'beyoncé', 'release', 'dangerously', 'in', 'love', '?']\n"
     ]
    }
   ],
   "source": [
    "print(toks_title)\n",
    "print(toks_context)\n",
    "print(toks_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'!',\n",
       " '\"',\n",
       " '#',\n",
       " '$',\n",
       " '%',\n",
       " '&',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " '+',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '/',\n",
       " ':',\n",
       " ';',\n",
       " '<',\n",
       " '=',\n",
       " '>',\n",
       " '?',\n",
       " '@',\n",
       " '[',\n",
       " '\\\\',\n",
       " ']',\n",
       " '^',\n",
       " '_',\n",
       " '`',\n",
       " '{',\n",
       " '|',\n",
       " '}',\n",
       " '~'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "set(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
